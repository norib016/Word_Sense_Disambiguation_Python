# -*- coding: utf-8 -*-
"""
Created on Wed Oct 30 10:05:34 2019

@author: Sree Nori
@date: 11/3/2019

1. Introduction of this project
The purpose of this project is to disambiguate word sense in a given sentence.

In the test data, there is a list of sentences including the word "line" or "lines". 
The sense could be either "product" or "phone". 

This project aims to idendify the sense of "line" or "lines" based on the training dataset. 

Example: 

- Following is the <context> from the test file. "lines" is the ambigous word. 
- This project is trying to identfy the sense (could be "product" or "phone") of "lines" in this sentense. 

<instance id="line-n.w8_106:13309:">
<context>
<s>But he doesn't apologize for the idea, because ZapMail was an effort to address a real and continuing problem.</s><s>Facsimile machines, which transmit documents over telephone <head>lines</head> , are eating into Federal's overnight business.</s>
</context>


2.  Features were used from the training dataset (from higher priority to lower priority):
    - Two words before and after the ambiguous word.
    - One word before and after the ambiguous word.
    - Two words before the ambiguous word.        
    - Two words after the ambiguous word.    
    - One word before the ambiguous word.
    - One word after the ambiguous word.
    
    Besides the above six features, a few words that highly related to "phone" were used to identify the ambiguous words.

3. Accuracy
    - My accuracy is 92.857%
    - Baseline accuracy is 42.857%
    
4. Confusion matrix
                           (predicted)
                        product    phone
            product        49        5
            phone           4        68
    
    - 49 products were correctly identified as products     
    - 5 products were misidentified as phones
    - 68 phones were correctly identified as phones
    - 4 phones were misidentified as products


5. Algorithm 
    -> 1) import train, test files based on user's input
    
    -> 2) extract instance ID, sense ID and the associated <context> from "line-train.xml"
    
    -> 3) preprocess the <context> and identify the ambigious word
    
    -> 4) extract six types of features, and create six feature lists based on the location of the ambigous word
             For example: (left word, ambigous word), (right word, ambigous word), (left word + right word, ambigous word)...
        
    -> 5) based on the above six feature lists, calculate the featue occurrence in terms of sense "product" and "phone", 
          store them in six dataframes. Each dataframe stores the features occurrence to sense "product" and "phone". 
              For example:
                            product    phone
                   access      2         1
                   the         5         4
    
    -> 6) extract instance ID and the ambiguous associated <context> from "line-test.xml"
    
    -> 7) iterate each <context>, extract six features based the location of ambigouous word, same as 4)
          iterate each feature and look up the associated dataframes created at 5) to calculate log-likelihood of P(product|feature)|P(phone|feature)
          and return the sense which has higher P(product|feature) and P(product|phone)
          -> if the feature is not exist, return sense "product", because it has higher occurrence than "phone" in training dataset
          -> if P(product|feature) = P(phone|feature), return sense "product"
    
    -> 8) compare the log-likelihood generated by each feature, choose the sense returned by the feature which has the highest log-likelihood
    
    -> 9) store all the identified senses in a list, adjust the format, print the list out and store it in a new file: my-line-answers.txt. 

6. How to run the scripts

- please place "decision-list.py" and "scorer.py" in the same directory
6.1 decision-list.py
    - Create a folder called "PA3" in the same directory with "decision-list.py"
    - Put these 3 files under the folder "PA3": line-train.xml, line-test.xml, line-answers.txt
    - Open terminator, for example: Windows "Command Prompt"
    - Navigate to the directory of "decision-list.py"
    - Enter "python decision-list.py line-train.xml line-test.xml my-decision-list.txt my-line-answers.txt" and execute the script
    Result: 
        - In the terminal, it will show answer tags of the test data.
          For example: <answer instance="line-n.w7_057:1203:" senseid ="phone"/>
          ...
          
        - It will create a file named "my-line-answers.txt" under the folder folder "PA3". It stores the answers(senses) for test data.
        - It will also create a file named "my-decision-list.txt" under the folder "PA3". It stores the features,
          the log-likelihood scores associated with the features. It will also store the sense that was predicted and the instance ID. 

6.2 scorer.py
    - Open terminator, for example: Windows "Command Prompt"
    - Navigate to the directory of "scorer.py"
    - Enter "python scorer.py my-line-answers.txt line-answers.txt" and execute the script
    Result: 
        - It will show the base line accuracy and my overall accuray 
        - It will also show the confusion matrix 

"""
import sys, random
import numpy as np
import pandas as pd
import nltk, re
#from nltk.stem import WordNetLemmatizer 
from nltk.tokenize import word_tokenize
import xml.etree.ElementTree as ET
import logging


log = "decision-list-log.txt"   # create a log file

logging.basicConfig(filename = log,level = logging.DEBUG, format = '%(message)s')
logging.info('$ script decision-list-log.txt')   # start logging


# store information from train file    
instanceId_list, senseId_list = [], []  
leftword, rightword = [], []
left2word, right2word = [], []
pairOffset1, pairOffset2 = [], []

leftword_df = pd.DataFrame()
rightword_df = pd.DataFrame()
left2word_df = pd.DataFrame()
right2word_df = pd.DataFrame()
pairOffset1_df = pd.DataFrame()
pairOffset2_df = pd.DataFrame()

# store information from test file
instanceIdTest = []
contentTest = []

# store the sense (either product or phone) with higher count value in training dataset
higher_sense = ''
    
# get system input and return inputs
def get_inputs(): 
    input_len = len(sys.argv) - 1    # subtract decision-list.py
    
    if input_len < 4:        # check whether there are less than 4 arguments, if not, exit the script
        print("Please enter at least 4 arguments.")
        sys.exit()
      
    train = str(sys.argv[1])             # get the 1st argument: line-train.xml
    test = str(sys.argv[2])              # get the 2nd argument: line-test.xml
    decision_list = str(sys.argv[3])     # get the 3rd argument: my-decision-list.txt
    answers = str(sys.argv[4])           # get the 4th argument: my-line-answers.txt

    return train, test, decision_list, answers

# get 1 word before the target word
def left_1(context):    
    list_of_words = context.split()
    if list_of_words.index("*STA") == 0:
        return "EOL"
    else:
        left_word = list_of_words[list_of_words.index("*STA") - 1]
        return left_word

# get 1 word after the target word
def right_1(context):    
    list_of_words = context.split()
    if list_of_words.index("*EOL") >= len(list_of_words) - 1:
        return "EOL"
    else: 
        right_word = list_of_words[list_of_words.index("*EOL") + 1]
        return right_word

# get 2 words before the target word
def left_2(context):    
    list_of_words = context.split()
    
    if list_of_words.index("*STA") <= 1:
        return "EOL"            
    else:                 
        left2 = list_of_words[list_of_words.index("*STA") - 2] + " " + list_of_words[list_of_words.index("*STA") - 1]
        return left2

# get 2 words after the target word
def right_2(context):    
    list_of_words = context.split()
    
    if list_of_words.index("*EOL") >= len(list_of_words) - 2:
        return "EOL"
    else:     
        right2 = list_of_words[list_of_words.index("*EOL") + 1] + " " + list_of_words[list_of_words.index("*EOL") + 2]
        return right2

# pre-process text
def text_process(text):
    context = str(text).lower()    
    context = context.replace("<s>"," ")
    context = context.replace("</s>", "")
    context = context.replace("<context>", "")
    context = context.replace("</context>", "") 
    context = context.replace(r'\n', "")
    context = context.replace(r'\\s', "")
    context = context.replace(r'%', "")
    context =  re.sub(r'(\{|\}|\,|\'|\")', '', context)  
    context = re.sub(r'(\?|\.|\!|\;)', ' ', context)
    context = re.sub(r'(\d+)', '65596', context)  # convert different numbers to a unique number
    
    # convert <head> and </head> to special character in order to get the targe ambiguous word               
    context = context.replace("<head>", "*STA ")  
    context = context.replace("</head>", " *EOL")
    
    return context

# convert list to dataframe                
def list_to_df(feature):
    phone_dict, product_dict = {}, {}
    
    for i, v in enumerate(feature): 
        if v[1] == "phone":
            if v[0] not in phone_dict:
                phone_dict[v[0]] = 2     # starts with 2, add 1 for smoothing.
            else:
                phone_dict[v[0]] += 1
                
        elif v[1] == "product":
            if v[0] not in product_dict:
                product_dict[v[0]] = 2
            else:
                product_dict[v[0]] += 1
                
    
    phone_df = pd.DataFrame(list(phone_dict.items()))
    phone_df.columns = ['feature', 'phone']
    phone_df = phone_df.set_index('feature')
    
    product_df = pd.DataFrame(list(product_dict.items()))
    product_df.columns = ['feature', 'product']
    product_df = product_df.set_index('feature')

    #df = pd.merge(phone_df, product_df, on = 'feature', how = 'outer')      
    df = pd.concat([phone_df, product_df], axis = 1, sort = False)
    df = df.replace(np.nan, 1)    # replace Nan with 1 to perform smoothing
        
    return df   

# create dataframe to store the features and their occurrences in terms of senses "phone" and "product"
def create_freq_df():   
    global leftword_df, rightword_df, left2word_df, right2word_df, pairOffset1_df, pairOffset2_df
    leftword_df = list_to_df(leftword) 
    #print(leftword_df)
    
    rightword_df = list_to_df(rightword)
    left2word_df = list_to_df(left2word)
    right2word_df = list_to_df(right2word)
    pairOffset1_df = list_to_df(pairOffset1)
    pairOffset2_df = list_to_df(pairOffset2)   
    #print(pairOffset2_df)    

# import the training dataset, built instance ID list and sense list
# iterate each sentence, create features for each ambiguous word, and calculate features probabilies associated with each sense
# create dataframe to store the features and their occurrences in terms of senses "phone" and "product"
def process_train(train):
    
    tree = ET.parse('./PA3/' + train)    # read xml file and stores it in a tree
    root = tree.getroot()
    lines = root.find('lexelt')
    
    for line in lines:        
        for info in line:        
            if((info.attrib) != {}):                
                instanceId_list.append((line.attrib)['id'])   # store instance ID
                senseId = (info.attrib)['senseid']            # store senseID
                senseId_list.append(senseId)
            else:
                text = ET.tostring(info)                
                context = text_process(text)
                
                # 1 word before the target word
                left1 = left_1(context)
                leftword.append((left1, senseId))
                
                # 1 word after the target word
                right1 = right_1(context)
                rightword.append((right1, senseId))
                
                # 1 word before and 1 word after
                pairOffset1.append((left1 + " " + right1, senseId))
                
                # 2 words before the target word
                left2 = left_2(context)
                left2word.append((left2, senseId))
                
                # 2 words after the target word
                right2 = right_2(context)
                right2word.append((right2, senseId))
                           
                # 2 words before and 2 words after the target word
                pairOffset2.append((left2 + " " + right2, senseId))
    # create dataframe to store the features and their occurrences in terms of senses "phone" and "product"
    create_freq_df() 
                                

# input and pre-process test file
def process_test(test):   
    tree = ET.parse('./PA3/' + test)    # read xml file and stores it in a tree
    root = tree.getroot()
    lines = root.find('lexelt')
    
    for line in lines:        
        instanceIdTest.append((line.attrib)['id'])
        for info in line:
            text = str(ET.tostring(info))            
            context = text_process(text)
            contentTest.append(context)  
            
# check the training dataset, identify whether product or phone has the higher count
def check_higher_sense():
    
    global higher_sense
        
    product_count = senseId_list.count('product')
    phone_count = senseId_list.count('phone')
    
    if (product_count > phone_count):
        higher_sense = "product"
    elif (product_count < phone_count):
        higher_sense = "phone"
    else:
        higher_sense = random.choice(["product","phone"])
    #print(higher_sense)
    
                        
# calculate each feature's probability based on senses: "product" and "phone" 
# and return the sense which get a higher probability
def calculate_prob(feature, order): 
    global higher_sense
    
    if(feature != "EOL"):            
        if order == 5:
            if feature in rightword_df.index:
                product_num = rightword_df.loc[feature, 'product']                
                phone_num = rightword_df.loc[feature, 'phone']
            else:
                product_num = 0
                phone_num = 0   
        elif order == 4:                
            if feature in leftword_df.index:
                product_num = leftword_df.loc[feature, 'product']                
                phone_num = leftword_df.loc[feature, 'phone']
            else:
                product_num = 0
                phone_num = 0
        elif order == 3:
            if feature in right2word_df.index:
                product_num = right2word_df.loc[feature, 'product']                
                phone_num = right2word_df.loc[feature, 'phone']
            else:
                product_num = 0
                phone_num = 0  
        elif order == 2:
            if feature in left2word_df.index:
                product_num = left2word_df.loc[feature, 'product']                
                phone_num = left2word_df.loc[feature, 'phone']
            else:
                product_num = 0
                phone_num = 0  
        elif order == 1:
            if feature in pairOffset1_df.index:
                product_num = pairOffset1_df.loc[feature, 'product']                
                phone_num = pairOffset1_df.loc[feature, 'phone']
            else:
                product_num = 0
                phone_num = 0
        elif order == 0:
            if feature in pairOffset2_df.index:
                product_num = pairOffset2_df.loc[feature, 'product']                
                phone_num = pairOffset2_df.loc[feature, 'phone']
            else:
                product_num = 0
                phone_num = 0
        
        if (product_num == phone_num):   # Checks if product and phone have the same count 
            prob = 0                     # if they have the same count, means can't identify it is product or phone
            wsd = higher_sense           # use the one has higher sense count in training dataset    
        else:        # calculate the log-likelihood
            prob = np.log(product_num / phone_num)

            if(product_num > phone_num):   # return the sense which has higher count in training dataset
                wsd = "product";
            else:
                wsd = "phone"   

    # if feature == "EOL" means feature does not exist, return the sense which has higher occurrence in training dataset
    else:
        prob = 0
        wsd = higher_sense  
    return abs(prob), wsd
 
# identify the sense based on all features' probabilities    
def wsd():    
    left1word_prob, right1word_prob = [], []
    left2word_prob, right2word_prob = [], []
    pairOffset1_prob, pairOffset2_prob = [], []

    wsd_list, prob_nest = [], []
    x = -1
    
    for content in contentTest:
        manual_sense = ''
        x += 1
                
        right1 = right_1(content)
        order = 5  
        prob, right1_wsd = calculate_prob(right1, order)
        right1word_prob.append(prob)
 
        left1 = left_1(content)
        order = 4   
        prob, left1_wsd = calculate_prob(left1, order)
        left1word_prob.append(prob)       
         
        right2 = right_2(content)               # 2 words after the target word
        order = 3
        prob, right2_wsd = calculate_prob(right2, order)
        right2word_prob.append(prob)    

        left2 = left_2(content)                 # 2 words before the target word
        order = 2
        prob, left2_wsd = calculate_prob(left2, order)
        left2word_prob.append(prob)        

        pairOffset1 = left1 + " " + right1      # 1 word before and 1 word after
        order = 1
        prob, pairOffset1_wsd = calculate_prob(pairOffset1, order)
        pairOffset1_prob.append(prob)    
                        
        pairOffset2 = left2 + " " + right2      # 2 words before and 2 words after the target word
        order = 0    # 0 means this feature has the highest priority, will check it first
        prob, pairOffset2_wsd = calculate_prob(pairOffset2, order)
        pairOffset2_prob.append(prob)               
        
        # store all features' probabilities in list
        prob_list = [pairOffset2_prob[x], pairOffset1_prob[x], left2word_prob[x], right2word_prob[x], left1word_prob[x], right1word_prob[x]]
    
        prob_nest.append(prob_list)             # build a nested list to store all features' probabilities in terms of all instance IDs
        
        max_value = max(prob_list)              # find the max probability
        max_index = prob_list.index(max_value)  # get the index of the maximum value
       
        # if sentence incluses words: call, calls, phone, then assign sense phone to the ambiguous words
        token = word_tokenize(content)
        if 'call' in token:
            manual_sense = 'phone'
        elif 'calls' in token:
            manual_sense = 'phone'
        elif 'phone' in token:
            manual_sense = 'phone'
        
        # identify which feature gives the highest probability
        if manual_sense != '':
            wsd_list.append(manual_sense)
        elif max_index == 0:
            wsd_list.append(pairOffset2_wsd)
        elif max_index == 1:
            wsd_list.append(pairOffset1_wsd)
        elif max_index == 2:
            wsd_list.append(left2_wsd)
        elif max_index == 3:
            wsd_list.append(right2_wsd)
        elif max_index == 4:
            wsd_list.append(left1_wsd)        
        elif max_index == 5:
            wsd_list.append(right1_wsd)
            
    return wsd_list, prob_nest

# create a dataframe to store log-likelihood and sense, and then write them to decision_list.txt
def create_decision_list(wsd_list, prob_nest, decision_list):
    col_names = ['pairOffset2','pairOffset1','left2word','right2word','left1word',' right1word']  
    decision_list_df = pd.DataFrame()   
    decision_list_df = pd.DataFrame(prob_nest,columns = col_names)  
    decision_list_df['Sense'] = wsd_list
    decision_list_df['instanceID'] = instanceIdTest
    
    with open(('./PA3/' + decision_list), 'w') as f:
        f.write(decision_list_df.to_string())
    f.close

# create a text file with instance id and associated senses in "line-answers.txt" format
def create_answers(wsd_list, answers):    
    my_answers = []
    
    print()
    
    for i in range(0,len(instanceIdTest)):
        answer = '<answer instance="' + instanceIdTest[i] + '" senseid ="' + wsd_list[i] + '"/>'
        print(answer)
        my_answers.append(answer)  
    
    with open(('./PA3/' + answers), 'w') as f:
        f.write('\n'.join(my_answers))
    f.close()    
    

def main():  
    
    train, test, decision_list, answers = get_inputs()  
    
    logging.info('$ python decision-list.py %s %s %s %s', str(train), str(test), str(decision_list), str(answers)) 
    
    process_train(train)      
    process_test(test)  
    check_higher_sense()
    wsd_list, prob_nest = wsd()
    create_decision_list(wsd_list, prob_nest, decision_list)
    create_answers(wsd_list, answers)

      
if __name__ == "__main__":
    main()
    